---
title: "Entrega Proyecto 2 "
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objetivo 

El objetivo principal de este encargo es crear un programa computacional que permita crear una lista de  reproducción de 3 horas de duración basándose en alguna canción de referencia. La base de datos incluye 447.622 canciones, con 36 de las variables descritas en la documentación de la API de Spotify

## Importar Librerias

```{r}
library(tidyverse)
library(cluster)
library(factoextra)
library(janitor)
library(lattice)
library(stats4)
library(flexclust)
library(ggdendro)
```

## Importar datos

```{r}
#setwd("C:/Users/Felipe/Documents/GitHub/Entregas_mineria_de_datos/Proyecto 2")
load(file="beats.RData")

summary(beats)
```

## Elección de variables

De las 36 columnas que entrega la base de datos, muchas de ellas no son necesarias ni utiles para el trabajo. Se procede a eliminar las columnas de:

- Artist_id: El nombre del artista es mas relevante para el objetivo del trabajo dado que se pretende analizar la playlist con un analisis visual y el nombre es mas amigable que el id para esto. 
- Album_id: El nombre del album es mas relevante para el objetivo del trabajo dado que se pretende analizar la playlist con un analisis visual y el nombre es mas amigable que el id para esto. 
- Album_type: Saber que la canción esta en un album no aytuda a identificiar similitudes entre las canciones
-Album release year:
- Album_release_day: Al ser una información demasiado especifica, es preferible utilizar solo el año de la canción para establecer similitudes. 
- Album_release_day_presition: No aporta información ademas de especificar que la canción fue liberada en un dia
-mode:
- Track_id: El nombre de la cancion es mas relevante para el objetivo del trabajo dado que se pretende analizar la playlist con un analisis visual y el nombre es mas amigable que el id para esto. 
- Analysis_url: No aporta info para buscar semejanzas 
- Time_signature:No aporta info para buscar semejanzas entre canciones en relación a su composición musical
- Disc_number: No aporta info para buscar semejanzas entre canciones en relación a su composición musical dado que es una información que apunta a la historia del artista. 
- Explicit: No es una información relevante para establecer semejanzas entre canciones.
- Track_href:  No es una información relevante para establecer semejanzas entre canciones.
- is_local: No es una información relevante para establecer semejanzas entre canciones.
- Track_preview_url: No es una información relevante para establecer semejanzas entre canciones.
- Track_number:  No es una información relevante para establecer semejanzas entre canciones.
- Type: No es una información relevante para establecer semejanzas entre canciones.
- Track_url: No es una información relevante para establecer semejanzas entre canciones.
- External_url_spotify: No es una información relevante para establecer semejanzas entre canciones.
-Key name:
-mode name:
key mode:

```{r}
beats=beats[,c(1,8:11,13:18,19,23,27,33)]
```

##Selección de Data

Debido a la gran cantidad de datos, algunas funciones y graficas saturan las capacidades del computador, por lo cual es necesario realizar una muestra. El procedimiento consiste en generar una muestra aleatoria simple sin reemplazo a traves de la función "sample()" con una cantidad de 20.000 datos. Es necesario validar que esta muestre represente a la base de datos asignada por lo cual se compararan las estadisticas de estas agrupaciones de datos a traves de la función "summary". 

```{r}
sample_index <- sample(1:nrow(beats),20000, replace = F)
sample_data = beats[sample_index,]

summary(sample_data)
```
Lo importante es comparar las variables numericas dado que estas son las variables elegidas.Como se puede observar, en ninguno de los parametros comparados (Minimo,Maximo,Median,Mean,Cuartiles) las diferencias superan las magnitudes del 0,01 por lo cual se asume que la muestra seleccionada reprsenta a la data total dado que son similares.

## Limpieza de datos

#Busqueda de datos faltantes

En primer lugar, para las observaciones que tengan datos faltantes, le asignamos el valor NA para eliminarlos en el siguiente paso. Luego, se revisa que no queden valores nulos. 
Podemos observar que solo la columna "album_release_year" cuenta con datos faltantes, por lo cual solo se trabaja esa columna. En el primer "sumarise_all" se muestran los NA encontrados por cada columna y en la segunda iteración de este comando se evidencia que la data seleccionada ya no contiene elementos faltantes. 

```{r}
sample_data[sample_data == ""] <- NA
sample_data %>%  summarise_all(funs(sum(is.na(.))))
#sample_data <- sample_data %>% filter(!(is.na(sample_data$album_release_year)))
sample_data %>% summarise_all(funs(sum(is.na(.))))


```

#Busqueda de datos duplicados

Asumiendo que la base de datos cuenta con una perfecta identificación de cada canción, el siguente algoritmo pretende buscar si dentro de la muestra seleccionada se encuentran canciones con su mismo ID. En caso de ser encontradas, se eliminan debido a que se considera una canción duplicada. 

Data actualizada pretende mantener un data frame sin escalar para luego ocupar las duraciones de las canciones reales, es decir, sin ser modificadas. 
```{r}
data_lista = sample_data[!duplicated(sample_data$track_id),]

data_actualizada=data_lista

```

##Analisis de Clusters

Se necesita identificar las variables que definen el comportamiento de las canciones para luego evaluar su semejanza o difeencia, por lo cual se utilizaran las variables:
- danceability
- energy
- key ?????????
- loudness
- spechiness
- acusticness
- instrumentalness
- liveness
- valance
- tempo
- duracicon_ms

Todas las variables ya son numericas por lo cual es mas facil escalarlas 

#Escalar Datos

La intención de escalar los datos es que estos se encuentren centrados en el 0 y con desviación estandar 1 

Dado que la función "scale" entrega el resultado como una matriz, se utiliza el argumento "as_tibble" para que el resultado sea un data frame. 
```{r}
var_a_utilizar_en_clusters = data_lista[,c(2:11,13)]
data_escalada_1=scale(var_a_utilizar_en_clusters) %>% as_tibble()
summary(data_escalada_1)

```
Se puede observar que la data se encuentra escalada dado que

#Implementacion de K Means

En primer lugar se planea identificar la mejor combinación a partir de la "regla del codo" y del "coeficiente de silueta". Con la regla del codo Asi se puede ver como evoluciona la suma de cuadrados intra-cluster en la medida que aumentamos el numero de k. Se deveria poder observar como disminuye la cohesión mientras aumenta el numero de klusters.Por otro lado, el coeficiente de silueta tambien ayuda a determinar el numero optimo de agrupamientos de clusters. 

```{r}
vector_codo_1 <- numeric(30)

for(k_codo_iteracion_1 in 1:30){
  modelo_codo_1 <- kmeans(data_escalada_1, centers = k_codo_iteracion_1)
  vector_codo_1[k_codo_iteracion_1] <- modelo_codo_1$tot.withinss
}

plot(vector_codo_1)

```
A simple vista no queda muy claro cual es el valor que genera el "codo" o quiebre en la recta por lo cual se complementa con el coeficiente de silueta.

# Coeficiente de silueta 

Se utiliza el coeficiente de silueta para determinar el mejor valor de K. 
```{r}
vector_silueta_1=numeric(20)
Buffer_1 = 0

for (k_silueta_iteracion_1 in 2:20){
  modelo_silueta_1 <- kmeans(data_escalada_1, centers = k_silueta_iteracion_1)
  variable_temporal_1 <- silhouette(modelo_silueta_1$cluster,dist(data_escalada_1))
  vector_silueta_1[k_silueta_iteracion_1] <- mean(variable_temporal_1[,3])
  
  if(vector_silueta_1[k_silueta_iteracion_1]>= Buffer_1){
    Buffer_1 = vector_silueta_1[k_silueta_iteracion_1]
    elección_k_iteracion_1 = k_silueta_iteracion_1
  }
}
tempDF_1=data.frame(CS=vector_silueta_1,K=c(1:20))


ggplot(tempDF_1, aes(x=K, y=CS)) + geom_line() + scale_x_continuous(breaks=c(1:30))

```

A partir de la regla del codo y del coeficiente de silueta se determina que el mejor K es el valor 3 dado que presenta un buen indice de cohesión. 
Se crea la variable "clus" para guardar el resultado del modelo

##Implementación de Kmeans Primera iteración 
BUSCAR AQUI LA MEJOR COMBINACION DE VARIABLES QUE MUESTREN EL CLUSTER
```{r}

modelo_k_means_1 <- kmeans(data_escalada_1, centers = elección_k_iteracion_1)

data_escalada_1$clus_iteracion_1 <- modelo_k_means_1$cluster %>% as.factor()
data_actualizada$clus_iteracion_1 <- modelo_k_means_1$cluster %>% as.factor()

ggplot(data_escalada_1, aes(energy,loudness, color=clus_iteracion_1 )) +  geom_point(alpha=0.5, show.legend = F) +  theme_bw()

```

#Evaluación de K Means

De acuerdo al estadistico de Hopkins, el cual mide la tendencia de los Klusters. Se realzia con una cantidad de 20 muestras 

Luego se validará mediante el limite de Cohesión y de separación 

No se realiza un estudio del indice de correlación dado que en la matriz de correlación  el sistema indica que "no se puede ubicar un vector de tamaño  244.2 Gb"

Se evita la evaluación visual dado que el algoritmo funciona con numeros generados aleatoriamente y la idea es que para cada iteración del algoritmo, este mismo determine cual es el mejor Cluster

```{r}

#Estadistico de Hopkins
Hopkins_1 <- get_clust_tendency(var_a_utilizar_en_clusters, n = 20, graph = FALSE)
print(Hopkins_1)

data_escalada_1 <- apply(data_escalada_1,2,as.numeric)

#Cohesion
print(modelo_k_means_1$tot.withinss)

#Separation
meanData_1 <- colMeans(data_escalada_1)
SSB_1 <- numeric(elección_k_iteracion_1) #Este valor y el que esta en el for DEPENDEN DE LA CANTIDAD DE CLUSTERS QUE DETERMINA EL MODELO. 
for (i_1 in 1:elección_k_iteracion_1){
  tempData_1 <- data_escalada_1[which(modelo_k_means_1$cluster==i_1),]
  SSB_1[i_1] <- nrow(tempData_1)*sum((meanData_1-colMeans(tempData_1))^2)
}
separation_1 = sum(SSB_1)

print(separation_1)

#Coeficiente de silueta
coefSilueta_1 <- silhouette(modelo_k_means_1$cluster,dist(data_escalada_1))
summary(coefSilueta_1)

#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSilueta_1) + coord_flip()
```
Un valor alto en el estadistico de Hopkins demuestra que existen agrupaciones en los datos.  

Un alto valor en la cohesion indica que tan cercanos se encuentran los integrantes de un determinado  cluster. Se pretende que este valor sea alto. 

El valor de la separación mide que tan alejados se encuentran los clusters frente a otros clusters. 

El análisis de siluetas mide qué tan bien se agrupa una observación y estima la distancia media entre los clústeres. Mientras mas grande sea el ancho medio de las siluetas, mejor agrupados se encuentran.

Debido a que la primera iteración del algoritmo K Means no genera clusters con caracteristicas que demuestran una semejanza alta, se decide tomar el mejor Cluster de la primera iteración de K Means y volver a generar Clusters a partir de la data de este cluster.

##Segunda iteración K Means

En primer lugar se genera un nuevo Data Frame con las canciones que en la iteración anterior fueron seleccionados en el mejor cluster. 

#Eleccion del mejor cluster de iteración pasada 

Si no pongo coefSilueta_1[,1] recorre tres veces la length de la variables porque esta tiene 3 columnas

```{r}
variable_1_anexa_para_seleccionar_cluster_1= 0
contador_anexa_para_seleccionar_cluster_1=0
variable_2_anexa_para_seleccionar_cluster_1=0
variable_3_anexa_para_seleccionar_cluster_1=0
variable_4_anexa_para_seleccionar_cluster_1=0

for(k_silueta_1 in 1:elección_k_iteracion_1){
    for (i_silueta_1 in 1:length(coefSilueta_1[,1]) ){
        if(coefSilueta_1[i_silueta_1,1]== k_silueta_1){
          variable_1_anexa_para_seleccionar_cluster_1=variable_1_anexa_para_seleccionar_cluster_1+coefSilueta_1[i_silueta_1,3]    
          contador_anexa_para_seleccionar_cluster_1=contador_anexa_para_seleccionar_cluster_1+1         
        }
    } 
    variable_2_anexa_para_seleccionar_cluster_1=variable_1_anexa_para_seleccionar_cluster_1/contador_anexa_para_seleccionar_cluster_1     
    print(variable_2_anexa_para_seleccionar_cluster_1) ##Esto solo es para validar que estoy calculando lo mismo que arriba
    
    if(variable_2_anexa_para_seleccionar_cluster_1 > variable_3_anexa_para_seleccionar_cluster_1){
       variable_3_anexa_para_seleccionar_cluster_1 = variable_2_anexa_para_seleccionar_cluster_1
       variable_4_anexa_para_seleccionar_cluster_1 = k_silueta_1      
    }
    variable_1_anexa_para_seleccionar_cluster_1= 0
    contador_anexa_para_seleccionar_cluster_1=0
    variable_2_anexa_para_seleccionar_cluster_1=0
}

print(variable_4_anexa_para_seleccionar_cluster_1) ##Esto solo es para validar que estoy determinando al mejor cluster bien


data_iteracion_2= data_actualizada[data_actualizada$clus_iteracion_1 == variable_4_anexa_para_seleccionar_cluster_1,]  ##Antes de variable 4 iba un 2 porque ese era le mejor cluster. Eso pasaba en estas dos filas
data_actualizada= data_actualizada[data_actualizada$clus_iteracion_1 == variable_4_anexa_para_seleccionar_cluster_1,]

```

#Escalar Datos

```{r}
data_escalada_2=data_iteracion_2[,c(2:11,13)]
data_escalada_2=scale(data_escalada_2) %>% as_tibble()
summary(data_escalada_2)

```

#Modelo del codo

```{r}
vector_codo_2 <- numeric(30)

for(k_codo_iteracion_2 in 1:30){
  modelo_codo_2 <- kmeans(data_escalada_2, centers = k_codo_iteracion_2)
  vector_codo_2[k_codo_iteracion_2] <- modelo_codo_2$tot.withinss
}

plot(vector_codo_2)

```
# Coeficiente de silueta 
```{r}
vector_silueta_2=numeric(20)
Buffer_2 = 0

for (k_silueta_iteracion_2 in 2:20){
  modelo_silueta_2 <- kmeans(data_escalada_2, centers = k_silueta_iteracion_2)
  variable_temporal_2 <- silhouette(modelo_silueta_2$cluster,dist(data_escalada_2))
  vector_silueta_2[k_silueta_iteracion_2] <- mean(variable_temporal_2[,3])
  
    if(vector_silueta_2[k_silueta_iteracion_2]>= Buffer_2){
    Buffer_2 = vector_silueta_2[k_silueta_iteracion_2]
    elección_k_iteracion_2 = k_silueta_iteracion_2
    }
}
tempDF_2=data.frame(CS=vector_silueta_2,K=c(1:20))

ggplot(tempDF_2, aes(x=K, y=CS)) + geom_line() + scale_x_continuous(breaks=c(1:30))

```
#Implementacion de la segunda iteración
```{r}

modelo_k_means_2 <- kmeans(data_escalada_2, centers = elección_k_iteracion_2)

data_escalada_2$clus_iteracion_2 <- modelo_k_means_2$cluster %>% as.factor()

#Esto agrega el valor de la segunda iteracion
data_iteracion_2$clus_iteracion_2 <- modelo_k_means_2$cluster %>% as.factor() #dudo si eso esta bien
data_actualizada$clus_iteracion_2 <- modelo_k_means_2$cluster %>% as.factor()

ggplot(data_escalada_2, aes(energy,loudness, color=clus_iteracion_2 )) +  geom_point(alpha=0.5, show.legend = F) +  theme_bw()

```
#Evaluación de segunda iteracion de K Means

Se incluye dentro del algoritmo una validación con respecto a la cohesión y la separación. Se espera que la separación aumente para demostrar una separación mayor entre los clusters mientras que la cogesión debe aumentar tambien, demostrando que dentro de cada cluster los elementos que lo incuyen se encuentran cercanos unos con otros. 

```{r}
#Estadistico de Hopkins
Hopkins_2 <- get_clust_tendency(var_a_utilizar_en_clusters, n = 20, graph = FALSE)
print(Hopkins_2)

data_escalada_2 <- apply(data_escalada_2,2,as.numeric)
 
#Cohesion
print(modelo_k_means_2$tot.withinss)

#Separation
meanData_2 <- colMeans(data_escalada_2)
SSB_2 <- numeric(elección_k_iteracion_2) #Este valor y el que esta en el for DEPENDEN DE LA CANTIDAD DE CLUSTERS QUE DETERMINA EL MODELO. 
for (i_2 in 1:elección_k_iteracion_2){
  tempData_2 <- data_escalada_2[which(modelo_k_means_2$cluster==i_2),]
  SSB_1[i_2] <- nrow(tempData_2)*sum((meanData_2-colMeans(tempData_2))^2)
}
separation_2 = sum(SSB_2)

print(separation_2)

#Coeficiente de silueta
coefSilueta_2 <- silhouette(modelo_k_means_2$cluster,dist(data_escalada_2))
summary(coefSilueta_2)

#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSilueta_2) + coord_flip()

#Comparación de Evaluaciones con iteración 1

if(modelo_k_means_2$tot.withinss > modelo_k_means_1$tot.withinss){
  print("La cohesión en la segunda iteración es mayor que en la primera, demostrando una mejora en el algoritmo")
}
if(modelo_k_means_2$tot.withinss < modelo_k_means_1$tot.withinss){
  print("La cohesión en la segunda iteración es menor que en la primera, demostrando que la iteración no benefició la diferenciación de clusters")
}

if(separation_2 > separation_1){
  print("La separación en la segunda iteración es mayor que en la primera, demostrando una mejora en el algoritmo")
}
if(separation_2 < separation_1){
  print("La separación en la segunda iteración es menor que en la primera, demostrando que la iteración no benefició la diferenciación de clusters")
}

```

##Tercera iteración K Means

En primer lugar se genera un nuevo Data Frame con las canciones que en la iteración anterior fueron seleccionados en el mejor cluster. 

#Eleccion del mejor cluster de iteración pasada 

```{r}
variable_1_anexa_para_seleccionar_cluster_2= 0
contador_anexa_para_seleccionar_cluster_2=0
variable_2_anexa_para_seleccionar_cluster_2=0
variable_3_anexa_para_seleccionar_cluster_2=0
variable_4_anexa_para_seleccionar_cluster_2=0

for(k_silueta_2 in 1:elección_k_iteracion_2){
    for (i_silueta_2 in 1:length(coefSilueta_2[,1]) ){
        if(coefSilueta_2[i_silueta_2,1]== k_silueta_2){
          variable_1_anexa_para_seleccionar_cluster_2=variable_1_anexa_para_seleccionar_cluster_2+coefSilueta_2[i_silueta_2,3]    
          contador_anexa_para_seleccionar_cluster_2=contador_anexa_para_seleccionar_cluster_2+1         
        }
    } 
    variable_2_anexa_para_seleccionar_cluster_2=variable_1_anexa_para_seleccionar_cluster_2/contador_anexa_para_seleccionar_cluster_2   
    print(variable_2_anexa_para_seleccionar_cluster_2) ##Esto solo es para validar que estoy calculando lo mismo que arriba
    
    if(variable_2_anexa_para_seleccionar_cluster_2 > variable_3_anexa_para_seleccionar_cluster_2){
       variable_3_anexa_para_seleccionar_cluster_2 = variable_2_anexa_para_seleccionar_cluster_2
       variable_4_anexa_para_seleccionar_cluster_2 = k_silueta_2     
    }
    variable_1_anexa_para_seleccionar_cluster_2= 0
    contador_anexa_para_seleccionar_cluster_2=0
    variable_2_anexa_para_seleccionar_cluster_2=0
}

print(variable_4_anexa_para_seleccionar_cluster_2) ##Esto solo es para validar que estoy determinando al mejor cluster bien

data_iteracion_3= data_actualizada[data_actualizada$clus_iteracion_2 == variable_4_anexa_para_seleccionar_cluster_2,]
data_actualizada= data_actualizada[data_actualizada$clus_iteracion_2 == variable_4_anexa_para_seleccionar_cluster_2,]

```

#Escalar Datos


```{r}
data_escalada_3=data_iteracion_3[,c(2:11,13)]
data_escalada_3=scale(data_escalada_3) %>% as_tibble()
summary(data_escalada_3)

```

#Modelo del codo

```{r}
vector_codo_3 <- numeric(30)

for(k_codo_iteracion_3 in 1:30){
  modelo_codo_3 <- kmeans(data_escalada_3, centers = k_codo_iteracion_3)
  vector_codo_3[k_codo_iteracion_3] <- modelo_codo_3$tot.withinss
}

plot(vector_codo_3)

```
# Coeficiente de silueta 
```{r}
vector_silueta_3=numeric(20)
Buffer_3 = 0

for (k_silueta_iteracion_3 in 2:20){
  modelo_silueta_3 <- kmeans(data_escalada_3, centers = k_silueta_iteracion_3)
  variable_temporal_3 <- silhouette(modelo_silueta_3$cluster,dist(data_escalada_3))
  vector_silueta_3[k_silueta_iteracion_3] <- mean(variable_temporal_3[,3])
  
    if(vector_silueta_3[k_silueta_iteracion_3]>= Buffer_3){
    Buffer_3 = vector_silueta_3[k_silueta_iteracion_3]
    elección_k_iteracion_3 = k_silueta_iteracion_3
  }
}
tempDF_3=data.frame(CS=vector_silueta_3,K=c(1:20))

ggplot(tempDF_3, aes(x=K, y=CS)) + geom_line() + scale_x_continuous(breaks=c(1:30))

```
#Implementacion de la tercera iteración
```{r}

modelo_k_means_3 <- kmeans(data_escalada_3, centers = elección_k_iteracion_3)

data_escalada_3$clus_iteracion_3 <- modelo_k_means_3$cluster %>% as.factor()

#Esto agrega el valor de la tercera iteracion
data_iteracion_3$clus_iteracion_3 <- modelo_k_means_3$cluster %>% as.factor()

data_actualizada$clus_iteracion_3 <- modelo_k_means_3$cluster %>% as.factor() #####

ggplot(data_escalada_3, aes(energy,loudness, color=clus_iteracion_3 )) +  geom_point(alpha=0.5, show.legend = F) +  theme_bw()

```
#Evaluación de tercera iteracion de K Means


```{r}

#Estadistico de Hopkins
Hopkins_3 <- get_clust_tendency(var_a_utilizar_en_clusters, n = 20, graph = FALSE)
print(Hopkins_3)

data_escalada_3 <- apply(data_escalada_3,2,as.numeric)
 
#Cohesion
print(modelo_k_means_3$tot.withinss)

#Separation
meanData_3 <- colMeans(data_escalada_3)
SSB_3 <- numeric(elección_k_iteracion_3) #Este valor y el que esta en el for DEPENDEN DE LA CANTIDAD DE CLUSTERS QUE DETERMINA EL MODELO. 
for (i_3 in 1:elección_k_iteracion_3){
  tempData_3 <- data_escalada_3[which(modelo_k_means_3$cluster==i_3),]
  SSB_1[i_3] <- nrow(tempData_3)*sum((meanData_3-colMeans(tempData_3))^2)
}
separation_3 = sum(SSB_3)

print(separation_3)

#Coeficiente de silueta
coefSilueta_3 <- silhouette(modelo_k_means_3$cluster,dist(data_escalada_3))
summary(coefSilueta_3)

#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSilueta_3) + coord_flip()

#Comparación de Evaluaciones con iteración 2

if(modelo_k_means_3$tot.withinss > modelo_k_means_2$tot.withinss){
  print("La cohesión en la tercera iteración es mayor que en la primera, demostrando una mejora en el algoritmo")
}
if(modelo_k_means_3$tot.withinss < modelo_k_means_3$tot.withinss){
  print("La cohesión en la tercera iteración es menor que en la primera, demostrando que la iteración no benefició la diferenciación de clusters")
}

if(separation_3 > separation_2){
  print("La separación en la tercera iteración es mayor que en la primera, demostrando una mejora en el algoritmo")
}
if(separation_3 < separation_2){
  print("La separación en la tercera iteración es menor que en la primera, demostrando que la iteración no benefició la diferenciación de clusters")
}




```
##Playlist a partir de K Means

#Selección del mejor Cluster de Ultima iteración

```{r}
variable_1_anexa_para_seleccionar_cluster_3= 0
contador_anexa_para_seleccionar_cluster_3=0
variable_2_anexa_para_seleccionar_cluster_3=0
variable_3_anexa_para_seleccionar_cluster_3=0
variable_4_anexa_para_seleccionar_cluster_3=0

for(k_silueta_3 in 1:elección_k_iteracion_3){
    for (i_silueta_3 in 1:length(coefSilueta_3[,1]) ){
        if(coefSilueta_3[i_silueta_3,1]== k_silueta_3){
          variable_1_anexa_para_seleccionar_cluster_3=variable_1_anexa_para_seleccionar_cluster_3+coefSilueta_3[i_silueta_3,3]    
          contador_anexa_para_seleccionar_cluster_3=contador_anexa_para_seleccionar_cluster_3+1         
        }
    } 
    variable_2_anexa_para_seleccionar_cluster_3=variable_1_anexa_para_seleccionar_cluster_3/contador_anexa_para_seleccionar_cluster_3 
    print(variable_2_anexa_para_seleccionar_cluster_3) ##Esto solo es para validar que estoy calculando lo mismo que arriba
    
    if(variable_2_anexa_para_seleccionar_cluster_3 > variable_3_anexa_para_seleccionar_cluster_3){
       variable_3_anexa_para_seleccionar_cluster_3 = variable_2_anexa_para_seleccionar_cluster_3
       variable_4_anexa_para_seleccionar_cluster_3 = k_silueta_3    
    }
    variable_1_anexa_para_seleccionar_cluster_3= 0
    contador_anexa_para_seleccionar_cluster_3=0
    variable_2_anexa_para_seleccionar_cluster_3=0
}

print(variable_4_anexa_para_seleccionar_cluster_3) ##Esto solo es para validar que estoy determinando al mejor cluster bien

dataframe_playlist= data_actualizada[data_iteracion_3$clus_iteracion_3 == variable_4_anexa_para_seleccionar_cluster_3,]
dataframe_playlist=dataframe_playlist[,c(1,13,14)]
```

#Generación de numeros aleatorios

```{r}
sample_index_playlist <- sample(1:nrow(dataframe_playlist),dim(dataframe_playlist), replace = F)

```

#Transformación del tiempo 
Pasarlos de ms a segundos y luego de segundos a minutos
```{r}
dataframe_playlist$duration_ms=dataframe_playlist$duration_ms*0.001/60
```
#Generación de playlist

```{r}
contador_de_tiempo=0
playlist_final=NULL
playlist_final= data.frame('artista','cancion')

for (k_playlist in 1:dim(dataframe_playlist)){
    variable_auxiliar_playlist=sample_index_playlist[k_playlist]
    print(dataframe_playlist[variable_auxiliar_playlist,c(1,3)]) #Esto es para checkear el dataframe de canciones
    
    playlist_final[k_playlist,1]=dataframe_playlist[variable_auxiliar_playlist,1]
    playlist_final[k_playlist,2]=dataframe_playlist[variable_auxiliar_playlist,3]
    
    contador_de_tiempo=contador_de_tiempo+dataframe_playlist$duration_ms[variable_auxiliar_playlist]
    if(contador_de_tiempo >= 180){
      view(playlist_final)
      break
    }
}
```

#Elección de canción y playlist asociada

```{r}


```


