---
title: "Entrega Proyecto 2"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objetivo 

El objetivo principal de este entregable es crear un programa computacional que permita crear una lista de reproducción de 3 horas de duración basándose en alguna canción de referencia. La base de datos incluye 447.622 canciones, con 36 de las variables descritas en la documentación de la API de Spotify.

El procedimiento consiste en generar una muestra aleatoria de la base de datos descrita, a la cual se le realizará una clusterización mediante K medias con una cantidad de clusters seleccionado por el estudio del coeficiente de silueta. Luego, se generará una nueva base de datos con las canciones que pertenecen al cluster con mayor agrupación resultante de la iteración anterior, al cual nuevamente se le realizará una clusterización mediante el algoritmo K Medias con una cantidad de cluster determinado por el coeficiente de silueta aplicado a la nueva data seleccionada. Por último, las canciones que pertenecen al cluster con mejor agrupación obtenido por esta nueva iteración, se les realizará una ultima clusterización mediante el algoritmo de K Medias con una cantidad de clusters determinado por el coeficiente de silueta ampliado a la nueva data seleccionada.

Las canciones resultantes de esta tercera iteración corresponderán a las canciones que podrían ser incluidas en la playlist. A continuación, de manera aleatoria se irán seleccionando canciones pertenecientes a este conjunto de canciones hasta completar las tres horas de duración requeridas. 

Por último, dentro de esta nueva pequeña agrupación de canciones, se seleccionará una de manera aleatoria y se concluirá que el resto de canciones que esta ultima acción seleccionó corresponden a una playlist con canciones similares a la canción elegida. 

La idea detrás del algoritmo de K Medias es agrupar las canciones que presenten una similitud en variables numéricas que describen el comportamiento de la canción, es decir, seleccionar canciones similares en su composición y sonido. 

Se utiliza este algoritmo dado que es un algoritmo de clasificación no supervisada (clusterización) que agrupa objetos en k grupos basándose en sus características. El agrupamiento se realiza minimizando la suma de distancias entre cada objeto y el centroide de su grupo o cluster, es decir, es bastante hábil guiándose en las semejanzas que se tiene en los datos, lo cual es super útil si se trata de buscar una playlist con canciones semejantes.

Es importante destacar que el código es totalmente autónomo, es decir, es capaz de adaptarse a cualquier conjunto aleatorio de canciones que se determinan y es capaz de decidir automáticamente la cantidad de clusters y el mejor cluster obtenido por cada iteración.Debido a lo anterior y para obtener resultados de una iteración en específico, se utilizó una semilla para mitigar la parte aleatoria de los resultados. 

## Importar Librerías

Se incorporarán al algoritmo las librarías que contienen las funciones que se utilizaran durante el funcionamiento del algoritmo.

```{r, message=FALSE}
library(tidyverse)
library(cluster)
library(factoextra)
library(janitor)
library(lattice)
library(stats4)
library(flexclust)
library(ggdendro)
library(knitr)
```

## Importar datos

Los datos son asignados y corresponden al csv denominado "beats", el cual contiene 447.622 canciones, con 36 de las variables descritas en la documentación de la API de Spotify. 

```{r}
setwd("C:/Users/Felipe/Documents/GitHub/Entregas_mineria_de_datos/Proyecto 2")
load(file="beats.RData")

summary(beats)
```

## Elección de variables

De las 36 columnas que entrega la base de datos, muchas de ellas no son necesarias ni útiles para crear una playlist. Se procede a eliminar las columnas de:

- Artist_id: El nombre del artista es más relevante para el objetivo del trabajo y representa la misma información pero de una manera visiblemente más agradable.

- Album_id: El nombre del álbum es más relevante para el objetivo del trabajo y representa la misma información pero de una manera visiblemente más agradable.

- Album_type: Saber que la canción está en un álbum no ayuda a identificar similitudes entre las canciones.

- Album release year: A pesar de que una similitud en las canciones puede ser el año de creación, esta no corresponde a una similitud en la composición de la canción por lo cual no se utilizará esta variable.

- Album_release_day:Es una información demasiado específica que no ayuda a generar similitudes en la composición o estilo de las canciones.

- Album_release_day_presition: No aporta información además de especificar que la canción fue liberada en un día.

- mode: Representa un valor binario que no es relevante para generar similitudes en la composición de las canciones.

- key: No se encontró información sobre lo que aporta esta variable, por lo cual se decide eliminarla.

- Track_id: El nombre de la canción es más relevante para el objetivo del trabajo y representa la misma información pero de una manera visiblemente más agradable.

- Analysis_url: No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- Time_signature:No aporta información para buscar semejanzas entre canciones en relación con su composición musical.

- Disc_number: No aporta información para buscar semejanzas entre canciones en relación con su composición musical dado que es una información que apunta a la historia del artista.

- Explicit: No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- Track_href: No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- is_local: No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- Track_preview_url: No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- Track_number: No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- Type: No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- Track_url: No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- External_url_spotify: No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- Key name:No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- mode name:No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

- key mode:No aporta información para buscar semejanzas entre las canciones en términos de su composición o estilo musical.

```{r}
beats=beats[,c(1,8:9,11,13:18,19,23,27,33)]
```

## Selección de Data

Debido a la gran cantidad de datos, algunas funciones y gráficas saturan las capacidades de procesamiento del computador, por lo cual se decidió implementar una muestra. El procedimiento consiste en generar una muestra aleatoria simple sin reemplazo a través de la función "sample()" con una cantidad de 20.000 datos. Es necesario validar que esta muestra represente a la base de datos asignada por lo cual se compararan las estadísticas de esta selección de datos con la data original a través de la función "summary".

```{r}
set.seed(2000000)
sample_index <- sample(1:nrow(beats),20000, replace = F)
sample_data = beats[sample_index,]

summary(sample_data)
```

Lo importante es comparar las variables numéricas dado que estas son las variables elegidas. Como se puede observar, en ninguno de los parámetros comparados (Mínimo, Maximo, Median,Mean, Cuartiles) las diferencias superan las magnitudes del 0,01 por lo cual se asume que la muestra seleccionada representa a la data total dado que son similares.

# Limpieza de datos

## Búsqueda de datos faltantes

En primer lugar, se revisa si hay casos faltantes. Para las observaciones que tengan datos faltantes, se le asigna el valor NA para eliminarlos en el siguiente paso. Luego, se revisa que no queden valores nulos. Tal como se observa a continuación, no se encontraron datos faltantes en la muestra seleccionada.

En iteraciones previas a la entrega final se determinó que solo la columna "album_release_year" cuenta con datos faltantes, pero no se va a trabajar con esa columna. De haber utilizado esa variable, para eliminar los datos faltantes se debe ocupar el comando "sample_data <- sample_data %&gt;% filter(!(is.na(sample_data$album_release_year)))" y de esta manera se excluyen de la base de datos las filas que en esa determinada columna cuenten con un NA.

```{r}
sample_data[sample_data == ""] <- NA
sample_data %>%  summarise_all(funs(sum(is.na(.))))
```

## Búsqueda de datos duplicados

Asumiendo que la base de datos cuenta con una perfecta identificación de cada canción, el siguiente comando pretende buscar si dentro de la muestra seleccionada se encuentran canciones con su mismo ID. En caso de ser encontradas, se eliminan debido a que se considera una canción duplicada.

La variable "data actualizada" pretende mantener un data frame sin escalar para luego ocupar las duraciones de las canciones reales, es decir, con datos idénticos a la base de datos sin alguna modificación.

```{r}
data_lista = sample_data[!duplicated(sample_data$track_id),]

data_actualizada=data_lista
```

# Análisis de Clúster

Se necesita identificar las variables que definen el comportamiento de las canciones para luego evaluar su semejanza o diferencia, por lo cual se utilizaran las variables:

- danceability: la cualidad o el estado de ser bailable
- energy: es la sensación de movimiento hacia adelante en la música, lo que mantiene al oyente comprometido y escuchando
- loudness: es una forma de medir los niveles de audio basada en la forma en que los humanos perciben el sonido
- spechiness: "La locuacidad detecta la presencia de palabras habladas en una canción". Si la locuacidad de una canción es superior a 0,66, probablemente esté compuesta por palabras habladas, una puntuación entre 0,33 y 0,66 es una canción que puede contener tanto música como palabras, y una puntuación inferior a 0,33 significa que la canción no tiene ninguna palabra.
- acusticness: Este valor describe cómo de acústica es una canción. 
- instrumentalness: Este valor representa la cantidad de voces en la canción.
- liveness: Este valor describe la probabilidad de que la canción se haya grabado con público en directo. Según la documentación oficial, "un valor superior a 0,8 proporciona una fuerte probabilidad de que la canción sea en directo".
- valance: Una medida de 0,0 a 1,0 que describe la positividad musical que transmite una pista. Las pistas con alta valencia suenan más positivas (por ejemplo, felices, alegres, eufóricas), mientras que las pistas con baja valencia suenan más negativas (por ejemplo, tristes, deprimidas, enfadadas)
- tempo: es la velocidad a la que se toca una pieza musical. Hay tres formas principales de comunicar el tempo a los músicos: BPM, terminología italiana y lenguaje moderno.

Todas las variables ya son numéricas por lo cual es más fácil escalarlas

## Escalar Datos

Dado que el algoritmo calcula la distancia euclidiana entre las variables, es necesario que las variables se muevan dentro de un rango acotado de valores puesto que si una variable se mueve entre 0 y 1 mientras que otra se mueve entre 0 y 1000, esta ultima influirá mucho más en el resultado porque pesa más.

La intención de escalar los datos es que estos se encuentren centrados en el 0 y con desviación estándar 1.

Dado que la función "scale" entrega el resultado como una matriz, se utiliza el argumento "as_tibble" para que el resultado sea un data frame. 

```{r}
var_a_utilizar_en_clusters = data_lista[,c(2:10)]
data_escalada_1=scale(var_a_utilizar_en_clusters) %>% as_tibble()
summary(data_escalada_1)

```

Se puede observar que la data se encuentra escalada dado que sus rangos máximos y mínimos son similares entre cada variable, lo cual permite que las distancias euclidianas de las variables con mayor dispersión valores no sesgan el resultado.

## Implementación de K Means

En primer lugar se planea identificar la mejor elección de clúster a partir de la "regla del codo" y del "coeficiente de silueta". De esta manera se pretende ver como evoluciona la suma de cuadrados intra-clúster en la medida que se aumenta el número de clúster elegidos. Se debería poder observar como disminuye la cohesión mientras aumenta el número de clúster. Por otro lado, el coeficiente de silueta también ayuda a determinar el número óptimo de agrupamientos de clúster.

```{r}
vector_codo_1 <- numeric(30)

for(k_codo_iteracion_1 in 1:30){
  modelo_codo_1 <- kmeans(data_escalada_1, centers = k_codo_iteracion_1)
  vector_codo_1[k_codo_iteracion_1] <- modelo_codo_1$tot.withinss
}

plot(vector_codo_1)
```

A simple vista no queda muy claro cuál es el valor que genera el "codo" o quiebre en la tendencia por lo cual se complementa con el coeficiente de silueta, el cual si determina de manera precisa cuál es la mejor elección de clúster dado que detecta cuál es el valor con mayor cohesión.

## Coeficiente de silueta

Se utiliza el coeficiente de silueta para determinar el mejor valor de K. Además, dentro del ciclo "for" se agrega una condición "if" que permite ir guardando el valor de clúster K que presentó el valor de cohesión más alto durante el ciclo para luego utilizar ese valor en la iteración del algoritmo de K Means.

```{r}
vector_silueta_1=numeric(20)
Buffer_1 = 0

for (k_silueta_iteracion_1 in 2:20){
  modelo_silueta_1 <- kmeans(data_escalada_1, centers = k_silueta_iteracion_1)
  variable_temporal_1 <- silhouette(modelo_silueta_1$cluster,dist(data_escalada_1))
  vector_silueta_1[k_silueta_iteracion_1] <- mean(variable_temporal_1[,3])
  
  if(vector_silueta_1[k_silueta_iteracion_1]>= Buffer_1){
    Buffer_1 = vector_silueta_1[k_silueta_iteracion_1]
    elección_k_iteracion_1 = k_silueta_iteracion_1
  }
}
tempDF_1=data.frame(CS=vector_silueta_1,K=c(1:20))

ggplot(tempDF_1, aes(x=K, y=CS)) + geom_line() + scale_x_continuous(breaks=c(1:30))
```

# Implementación de K Means Primera iteración

Se crea el modelo de K Means sobre la data elegida y con la cantidad de clúster almacenado en la variable "elección_k_iteracion_1", la cual contiene la mejor cantidad de clúster para utilizar en el modelo de K Means. A través del modelo se almacena en el data frame "data_actualizada" el valor del clúster al cual pertenece cada una de las canciones a partir de esta primera iteración del modelo.

El resultado se representa a través del siguiente Plot, sin embargo, es importante mencionar que esta es una de las tantas posibles combinaciones de variables que ilustran la agrupación de clúster resultante de esta iteración.

```{r}
modelo_k_means_1 <- kmeans(data_escalada_1, centers = elección_k_iteracion_1)

data_escalada_1$clus_iteracion_1 <- modelo_k_means_1$cluster %>% as.factor()
data_actualizada$clus_iteracion_1 <- modelo_k_means_1$cluster %>% as.factor()

ggplot(data_escalada_1, aes(energy,danceability, color=clus_iteracion_1 )) +  geom_point(alpha=0.5, show.legend = F) +  theme_bw()
```

## Evaluación de K Means

De acuerdo al estadístico de Hopkins, el cual mide la tendencia de los Klusters. Se realiza con una cantidad de 20 muestras. Luego se validará mediante el límite de Cohesión y de separación.

Se evita la evaluación visual dado que el algoritmo funciona con números generados aleatoriamente, es decir, en cada iteración se genera una diferente agrupación y cantidad de clúster, a los cuales el algoritmo por su cuenta debe decidir cuál es el mejor.

En primer lugar tenemos el estadístico de Hopkins:

```{r}
Hopkins_1 <- get_clust_tendency(var_a_utilizar_en_clusters, n = 20, graph = FALSE)
Hopkins_1
```

Un valor alto en el estadístico de Hopkins demuestra que existen agrupaciones en los datos.

Luego se estudia la Cohesión:

```{r}
data_escalada_1 <- apply(data_escalada_1,2,as.numeric)
modelo_k_means_1$tot.withinss
```

Un alto valor en la cohesión indica que tan cercanos se encuentran los integrantes de un determinado clúster. Se pretende que este valor sea alto.

A continuación se estudia la Separación:

```{r}
meanData_1 <- colMeans(data_escalada_1)
SSB_1 <- numeric(elección_k_iteracion_1) #Este valor y el que esta en el for DEPENDEN DE LA CANTIDAD DE CLUSTERS QUE DETERMINA EL MODELO. 
for (i_1 in 1:elección_k_iteracion_1){
  tempData_1 <- data_escalada_1[which(modelo_k_means_1$cluster==i_1),]
  SSB_1[i_1] <- nrow(tempData_1)*sum((meanData_1-colMeans(tempData_1))^2)
}
separation_1 = sum(SSB_1)
separation_1
```

El valor de la separación mide que tan alejados se encuentran los clúster frente a otros clúster.

Y finalmente se evalúa y grafica el coeficiente de silueta:

```{r}
coefSilueta_1 <- silhouette(modelo_k_means_1$cluster,dist(data_escalada_1))
summary(coefSilueta_1)
fviz_silhouette(coefSilueta_1) + coord_flip()
```

El análisis de siluetas mide qué tan bien se agrupa una observación y estima la distancia media entre los clústeres. Mientras más grande sea el ancho medio de las siluetas, mejor agrupados se encuentran.

Debido a que la primera iteración del algoritmo K Means no genera clúster con características que demuestran una semejanza alta, se decide tomar el mejor clúster de la primera iteración de K Means y volver a generar clúster a partir de la data de este clúster. Se determina que "el mejor clúster" es aquel que tenga una mayor agrupación de datos.

# Segunda iteración K Means

## Elección del mejor clúster de iteración pasada

En primer lugar, se genera un nuevo Data Frame con las canciones que en la iteración anterior fueron seleccionados en el mejor clúster. Para lograr esto se debe encontrar cuál fue el mejor clúster y por ende se utiliza el siguiente código, el cual finaliza con la elección del mejor clúster en la variable "variable_4_anexa_para_seleccionar_cluster_1".

Su funcionamiento es calcular de manera mecánica la columna que el gráfico anterior expone como "ave.sil.widt". Esta variable indica la agrupación en promedio que tuvo cada clúster. Para lograr este resultado, se recorre toda la variable "CoefSilueta_1", la cual contiene la agrupación y el clúster al cual cada cancón pertenece. Almacenando cada uno de estos valores por cada clúster y luego dividiendo por la cantidad de canciones que pertenecen a ese clúster se obtiene el valor en estudio. Para finalizar, se utiliza la segunda condición "if" para guardar el valor del clúster que presentó el mayor valor de agrupación.

Finalmente, en "data_iteracion_2" se almacenan las canciones presentes en "data_actualizada" que fueron seleccionadas en el clúster seleccionado en la iteración de K Medias anterior.

Si no se introduce el comando "coefSilueta_1[,1]" se recorre tres veces el largo de la variable porque esta tiene 3 columnas.

```{r}
variable_1_anexa_para_seleccionar_cluster_1= 0
contador_anexa_para_seleccionar_cluster_1=0
variable_2_anexa_para_seleccionar_cluster_1=0
variable_3_anexa_para_seleccionar_cluster_1=0
variable_4_anexa_para_seleccionar_cluster_1=0

for(k_silueta_1 in 1:elección_k_iteracion_1){
    for (i_silueta_1 in 1:length(coefSilueta_1[,1]) ){
        if(coefSilueta_1[i_silueta_1,1]== k_silueta_1){
          variable_1_anexa_para_seleccionar_cluster_1=variable_1_anexa_para_seleccionar_cluster_1+coefSilueta_1[i_silueta_1,3]    
          contador_anexa_para_seleccionar_cluster_1=contador_anexa_para_seleccionar_cluster_1+1         
        }
    } 
    variable_2_anexa_para_seleccionar_cluster_1=variable_1_anexa_para_seleccionar_cluster_1/contador_anexa_para_seleccionar_cluster_1     
    
    if(variable_2_anexa_para_seleccionar_cluster_1 > variable_3_anexa_para_seleccionar_cluster_1){
       variable_3_anexa_para_seleccionar_cluster_1 = variable_2_anexa_para_seleccionar_cluster_1
       variable_4_anexa_para_seleccionar_cluster_1 = k_silueta_1      
    }
    variable_1_anexa_para_seleccionar_cluster_1= 0
    contador_anexa_para_seleccionar_cluster_1=0
    variable_2_anexa_para_seleccionar_cluster_1=0
}

data_iteracion_2= data_actualizada[data_actualizada$clus_iteracion_1 == variable_4_anexa_para_seleccionar_cluster_1,]  
data_actualizada= data_actualizada[data_actualizada$clus_iteracion_1 == variable_4_anexa_para_seleccionar_cluster_1,]
```

A forma de comprobación, dada la iteración anterior, el mejor clúster elegido fue el clúster número:

```{r}
variable_4_anexa_para_seleccionar_cluster_1
```

## Escalar Datos

La intención de escalar los datos es que estos se encuentren centrados en el 0 y con desviación estándar 1.

Al igual que antes, dado que la función "scale" entrega el resultado como una matriz, se utiliza el argumento "as_tibble" para que el resultado sea un data frame.

```{r}
data_escalada_2=data_iteracion_2[,c(2:10)]
data_escalada_2=scale(data_escalada_2) %>% as_tibble()
summary(data_escalada_2)

```

Se puede observar que la data se encuentra escalada dado que sus rangos máximos y mínimos son similares entre cada variable, lo cual permite que las distancias euclidianas de las variables con mayor dispersión valores no sesgan el resultado.

## Regla del codo

De manera similar a la iteración anterior, se utiliza la regla del codo para observar el comportamiento de la posible elección de clúster y se utiliza el coeficiente de silueta para determinar la mejor cantidad de clúster dada la nueva data seleccionada.

```{r}
vector_codo_2 <- numeric(30)

for(k_codo_iteracion_2 in 1:30){
  modelo_codo_2 <- kmeans(data_escalada_2, centers = k_codo_iteracion_2)
  vector_codo_2[k_codo_iteracion_2] <- modelo_codo_2$tot.withinss
}

plot(vector_codo_2)
```

## Coeficiente de silueta

La cantidad de clúster que agrupa de mejor manera los datos queda almacenada en la variable "elección_k_iteracion_2" en la condición "if". 

```{r}
vector_silueta_2=numeric(20)
Buffer_2 = 0

for (k_silueta_iteracion_2 in 2:20){
  modelo_silueta_2 <- kmeans(data_escalada_2, centers = k_silueta_iteracion_2)
  variable_temporal_2 <- silhouette(modelo_silueta_2$cluster,dist(data_escalada_2))
  vector_silueta_2[k_silueta_iteracion_2] <- mean(variable_temporal_2[,3])
  
    if(vector_silueta_2[k_silueta_iteracion_2]>= Buffer_2){
    Buffer_2 = vector_silueta_2[k_silueta_iteracion_2]
    elección_k_iteracion_2 = k_silueta_iteracion_2
    }
}
tempDF_2=data.frame(CS=vector_silueta_2,K=c(1:20))

ggplot(tempDF_2, aes(x=K, y=CS)) + geom_line() + scale_x_continuous(breaks=c(1:30))

```

## Implementación de la segunda iteración

Se crea el modelo de K Means sobre la nueva data seleccionada y con la cantidad de clúster almacenado en la variable "elección_k_iteracion_2", la cual contiene la mejor cantidad de clúster para utilizar en el modelo de K Means. A través del modelo se almacena en el data frame "data_actualizada" el valor del clúster al cual pertenece cada una de las canciones a partir de esta segunda iteración del modelo.

El resultado se representa a través del siguiente Plot, sin embargo, es importante volver a mencionar que esta es una de las tantas posibles combinaciones de variables que ilustran la agrupación de clúster resultante de esta iteración.

```{r}

modelo_k_means_2 <- kmeans(data_escalada_2, centers = elección_k_iteracion_2)

data_escalada_2$clus_iteracion_2 <- modelo_k_means_2$cluster %>% as.factor()

#Esto agrega el valor de la segunda iteracion
data_iteracion_2$clus_iteracion_2 <- modelo_k_means_2$cluster %>% as.factor() #dudo si eso esta bien
data_actualizada$clus_iteracion_2 <- modelo_k_means_2$cluster %>% as.factor()

ggplot(data_escalada_2, aes(energy,danceability, color=clus_iteracion_2 )) +  geom_point(alpha=0.5, show.legend = F) +  theme_bw()

```

## Evaluación de segunda iteración de K Means

Para evaluar la nueva iteración, en primer lugar se utilizará el estadístico de Hopkins

```{r}
Hopkins_2 <- get_clust_tendency(var_a_utilizar_en_clusters, n = 20, graph = FALSE)
Hopkins_2
```
Un valor alto en el estadístico de Hopkins demuestra que existen agrupaciones en los datos.

Luego, se utilizará la cohesión:

```{r}
data_escalada_2 <- apply(data_escalada_2,2,as.numeric)
modelo_k_means_2$tot.withinss
```
Un alto valor en la cohesión indica que tan cercanos se encuentran los integrantes de un determinado clúster. Se pretende que este valor sea alto.

A continuación se valida la separación entre clúster:

```{r}
meanData_2 <- colMeans(data_escalada_2)
SSB_2 <- numeric(elección_k_iteracion_2) #Este valor y el que esta en el for DEPENDEN DE LA CANTIDAD DE CLUSTERS QUE DETERMINA EL MODELO. 
for (i_2 in 1:elección_k_iteracion_2){
  tempData_2 <- data_escalada_2[which(modelo_k_means_2$cluster==i_2),]
  SSB_1[i_2] <- nrow(tempData_2)*sum((meanData_2-colMeans(tempData_2))^2)
}
separation_2 = sum(SSB_2)

separation_2
```

El valor de la separación mide que tan alejados se encuentran el clúster frente a otro clúster.

Y finalmente se utiliza el coeficiente de silueta:

```{r}
coefSilueta_2 <- silhouette(modelo_k_means_2$cluster,dist(data_escalada_2))
summary(coefSilueta_2)

fviz_silhouette(coefSilueta_2) + coord_flip()
```

El análisis de siluetas mide qué tan bien se agrupa una observación y estima la distancia media entre los clústeres. Mientras más grande sea el ancho medio de las siluetas, mejor agrupados se encuentran.

A pesar de que el éndice expuesto en el gráfico es menor al de la primera iteración, se puede observar que por lo menos un clúster demuestra tener un tamaño y agrupación significativamente mayor al resto. 

# Tercera iteración K Means

## Elección del mejor clúster de iteración pasada

Se genera un nuevo Data Frame con las canciones que en la iteración anterior fueron seleccionados en el mejor clúster. Para lograr esto se debe encontrar cuál fue el mejor clúster y por ende se utiliza el siguiente código, el cual finaliza con la elección del mejor clúster en la variable "variable_4_anexa_para_seleccionar_cluster_2".

El funcionamiento es idéntico al ya explicado, por lo cual este código no volverá a ser explicado.

Finalmente, en "data_iteracion_3" se almacenan las canciones presentes en "data_actualizada" que fueron seleccionadas en el clúster seleccionado en la iteración de K Medias anterior.

```{r}
variable_1_anexa_para_seleccionar_cluster_2= 0
contador_anexa_para_seleccionar_cluster_2=0
variable_2_anexa_para_seleccionar_cluster_2=0
variable_3_anexa_para_seleccionar_cluster_2=0
variable_4_anexa_para_seleccionar_cluster_2=0

for(k_silueta_2 in 1:elección_k_iteracion_2){
    for (i_silueta_2 in 1:length(coefSilueta_2[,1]) ){
        if(coefSilueta_2[i_silueta_2,1]== k_silueta_2){
          variable_1_anexa_para_seleccionar_cluster_2=variable_1_anexa_para_seleccionar_cluster_2+coefSilueta_2[i_silueta_2,3]    
          contador_anexa_para_seleccionar_cluster_2=contador_anexa_para_seleccionar_cluster_2+1         
        }
    } 
    variable_2_anexa_para_seleccionar_cluster_2=variable_1_anexa_para_seleccionar_cluster_2/contador_anexa_para_seleccionar_cluster_2   
    
    if(variable_2_anexa_para_seleccionar_cluster_2 > variable_3_anexa_para_seleccionar_cluster_2){
       variable_3_anexa_para_seleccionar_cluster_2 = variable_2_anexa_para_seleccionar_cluster_2
       variable_4_anexa_para_seleccionar_cluster_2 = k_silueta_2     
    }
    variable_1_anexa_para_seleccionar_cluster_2= 0
    contador_anexa_para_seleccionar_cluster_2=0
    variable_2_anexa_para_seleccionar_cluster_2=0
}

data_iteracion_3= data_actualizada[data_actualizada$clus_iteracion_2 == variable_4_anexa_para_seleccionar_cluster_2,]
data_actualizada= data_actualizada[data_actualizada$clus_iteracion_2 == variable_4_anexa_para_seleccionar_cluster_2,]

```
Nuevamente a forma de comprobación, dada la iteración anterior, el mejor cluster elegido fue el cluster numero: 

```{r}
variable_4_anexa_para_seleccionar_cluster_2
```

## Escalar Datos

La intención de escalar los datos es que estos se encuentren centrados en el 0 y con desviación estándar 1.

Al igual que antes, dado que la función "scale" entrega el resultado como una matriz, se utiliza el argumento "as_tibble" para que el resultado sea un data frame.

```{r}
data_escalada_3=data_iteracion_3[,c(2:10)]
data_escalada_3=scale(data_escalada_3) %>% as_tibble()
summary(data_escalada_3)

```

Se puede observar que la data se encuentra escalada dado que sus rangos máximos y mínimos son similares entre cada variable, lo cual permite que las distancias euclidianas de las variables con mayor dispersión valores no sesgan el resultado.

## Modelo del codo

Finalmente y de manera idéntica a las dos iteraciones anteriores, se utiliza la regla del codo para observar el comportamiento de la posible elección de clúster y se utiliza el coeficiente de silueta para determinar la mejor cantidad de clúster dada la nueva data seleccionada.

```{r}
vector_codo_3 <- numeric(30)

for(k_codo_iteracion_3 in 1:30){
  modelo_codo_3 <- kmeans(data_escalada_3, centers = k_codo_iteracion_3)
  vector_codo_3[k_codo_iteracion_3] <- modelo_codo_3$tot.withinss
}

plot(vector_codo_3)

```

## Coeficiente de silueta

La cantidad de clúster que agrupa de mejor manera los datos queda almacenada en la variable "elección_k_iteracion_3".

```{r}
vector_silueta_3=numeric(20)
Buffer_3 = 0

for (k_silueta_iteracion_3 in 2:20){
  modelo_silueta_3 <- kmeans(data_escalada_3, centers = k_silueta_iteracion_3)
  variable_temporal_3 <- silhouette(modelo_silueta_3$cluster,dist(data_escalada_3))
  vector_silueta_3[k_silueta_iteracion_3] <- mean(variable_temporal_3[,3])
  
    if(vector_silueta_3[k_silueta_iteracion_3]>= Buffer_3){
    Buffer_3 = vector_silueta_3[k_silueta_iteracion_3]
    elección_k_iteracion_3 = k_silueta_iteracion_3
  }
}
tempDF_3=data.frame(CS=vector_silueta_3,K=c(1:20))

ggplot(tempDF_3, aes(x=K, y=CS)) + geom_line() + scale_x_continuous(breaks=c(1:30))

```

## Implementación de la tercera iteración

Se crea el modelo de K Means sobre la nueva data seleccionada y con la cantidad de clúster almacenado en la variable "elección_k_iteracion_3", la cual contiene la mejor cantidad de clúster para utilizar en el modelo de K Means. A través del modelo se almacena en el data frame "data_actualizada" el valor del clúster al cual pertenece cada una de las canciones a partir de esta segunda iteración del modelo.

El resultado se representa a través del siguiente Plot, sin embargo, es importante mencionar por última vez que esta es una de las tantas posibles combinaciones de variables que ilustran la agrupación de clúster resultante de esta iteración.

```{r}

modelo_k_means_3 <- kmeans(data_escalada_3, centers = elección_k_iteracion_3)

data_escalada_3$clus_iteracion_3 <- modelo_k_means_3$cluster %>% as.factor()
data_iteracion_3$clus_iteracion_3 <- modelo_k_means_3$cluster %>% as.factor()
data_actualizada$clus_iteracion_3 <- modelo_k_means_3$cluster %>% as.factor() 

ggplot(data_escalada_3, aes(energy,danceability, color=clus_iteracion_3 )) +  geom_point(alpha=0.5, show.legend = F) +  theme_bw()

```

## Evaluación de tercera iteración de K Means

De acuerdo al estadístico de Hopkins, el cual mide la tendencia de los Klusters. Se realiza con una cantidad de 20 muestras. Luego se validará mediante el límite de Cohesión y de separación.

Se evita la evaluación visual dado que el algoritmo funciona con números generados aleatoriamente, es decir, en cada iteración se genera una diferente agrupación y cantidad de clúster, a los cuales el algoritmo por su cuenta debe decidir cuál es el mejor.

En primer lugar tenemos el estadístico de Hopkins:

```{r}
Hopkins_3 <- get_clust_tendency(var_a_utilizar_en_clusters, n = 20, graph = FALSE)
Hopkins_3
```

Un valor alto en el estadístico de Hopkins demuestra que existen agrupaciones en los datos.

Luego está la cohesión:

```{r}
data_escalada_3 <- apply(data_escalada_3,2,as.numeric)
modelo_k_means_3$tot.withinss

```

Un alto valor en la cohesión indica que tan cercanos se encuentran los integrantes de un determinado clúster. Se pretende que este valor sea alto.

A continuación se realiza el estudio de la separación:

```{r}
meanData_3 <- colMeans(data_escalada_3)
SSB_3 <- numeric(elección_k_iteracion_3) #Este valor y el que esta en el for DEPENDEN DE LA CANTIDAD DE CLUSTERS QUE DETERMINA EL MODELO. 
for (i_3 in 1:elección_k_iteracion_3){
  tempData_3 <- data_escalada_3[which(modelo_k_means_3$cluster==i_3),]
  SSB_1[i_3] <- nrow(tempData_3)*sum((meanData_3-colMeans(tempData_3))^2)
}
separation_3 = sum(SSB_3)

separation_3
```

El valor de la separación mide que tan alejados se encuentran los clúster frente a otros clúster.

Y finalmente se utiliza el coeficiente de silueta:

```{r}
coefSilueta_3 <- silhouette(modelo_k_means_3$cluster,dist(data_escalada_3))
summary(coefSilueta_3)

fviz_silhouette(coefSilueta_3) + coord_flip()
```

El análisis de siluetas mide qué tan bien se agrupa una observación y estima la distancia media entre los clústeres. Mientras más grande sea el ancho medio de las siluetas, mejor agrupados se encuentran.

El gran cluster evidenciado en la anterior iteración se compone de esta cantidad de cluster dentro de el. Nuevamente se puede observar que uno de ellos destaca por sobre el resto con un gran tamaño y una ausencia de datos atipicos. Se puede concluir que este debe ser el cluster con los elementos mas semejantes entre ellos por lo cual este debe ser el cluster del cual se determinará la playlist. 

# Playlist a partir de K Means

## Selección del mejor Clúster de Última iteración

Se genera un nuevo Data Frame con las canciones que en la iteración anterior fueron seleccionados en el mejor clúster. Para lograr esto se debe encontrar cuál fue el mejor clúster y por ende se utiliza el siguiente código, el cual finaliza con la elección del mejor clúster en la variable "variable_4_anexa_para_seleccionar_cluster_3".

El funcionamiento es idéntico al ya explicado, por lo cual este código no volverá a ser explicado.

Finalmente, en "dataframe_playlist" se almacenan las canciones presentes en "data_actualizada" que fueron seleccionadas en el clúster seleccionado en la iteración de K Medias anterior.

```{r}
variable_1_anexa_para_seleccionar_cluster_3= 0
contador_anexa_para_seleccionar_cluster_3=0
variable_2_anexa_para_seleccionar_cluster_3=0
variable_3_anexa_para_seleccionar_cluster_3=0
variable_4_anexa_para_seleccionar_cluster_3=0

for(k_silueta_3 in 1:elección_k_iteracion_3){
    for (i_silueta_3 in 1:length(coefSilueta_3[,1]) ){
        if(coefSilueta_3[i_silueta_3,1]== k_silueta_3){
          variable_1_anexa_para_seleccionar_cluster_3=variable_1_anexa_para_seleccionar_cluster_3+coefSilueta_3[i_silueta_3,3]    
          contador_anexa_para_seleccionar_cluster_3=contador_anexa_para_seleccionar_cluster_3+1         
        }
    } 
    variable_2_anexa_para_seleccionar_cluster_3=variable_1_anexa_para_seleccionar_cluster_3/contador_anexa_para_seleccionar_cluster_3 
    
    if(variable_2_anexa_para_seleccionar_cluster_3 > variable_3_anexa_para_seleccionar_cluster_3){
       variable_3_anexa_para_seleccionar_cluster_3 = variable_2_anexa_para_seleccionar_cluster_3
       variable_4_anexa_para_seleccionar_cluster_3 = k_silueta_3    
    }
    variable_1_anexa_para_seleccionar_cluster_3= 0
    contador_anexa_para_seleccionar_cluster_3=0
    variable_2_anexa_para_seleccionar_cluster_3=0
}

dataframe_playlist= data_actualizada[data_iteracion_3$clus_iteracion_3 == variable_4_anexa_para_seleccionar_cluster_3,]
dataframe_playlist=dataframe_playlist[,c(1,12,13)]
```

Finalmente a forma de comprobación, dada la iteración anterior, el mejor clúster elegido fue el clúster número:

```{r}
variable_4_anexa_para_seleccionar_cluster_3
```

## Generación de números aleatorios

Para poder generar una especie de "shuffle" en la playlist, se realizará una lista con valores aleatorios para seleccionar canciones del dataframe de canciones que fueron determinadas del análisis de clúster, es decir, de todas las canciones que presentaron las más altas similitudes se generará una lista para seleccionar de manera aleatoria a alguna de ellas.

```{r}
sample_index_playlist <- sample(1:nrow(dataframe_playlist),dim(dataframe_playlist), replace = F)
```

## Transformación del tiempo

La duración presente en la base de datos original se encuentra en milisegundos (ms) por lo cual mediante el siguiente código se transformará a segundos y luego a minutos.

```{r}
dataframe_playlist$duration_ms=dataframe_playlist$duration_ms*0.001/60
```

## Generación de playlist

Con un ciclo que recorre desde el valor 1 hasta el valor del largo del dataframe de canciones que fueron seleccionadas por la clusterización, se seleccionaran por orden las canciones que la lista de números aleatorios determinó hasta completar un tiempo acumulado de 3 horas para así completar el requisito del entregable.

Se ocupa una condición de mayor o igual a 180 porque dado que el requisito son 3 horas, las cuales traducidas a minutos corresponden a 180 minutos. 

```{r,message=FALSE}
contador_de_tiempo=0
playlist_semi_final=NULL
playlist_semi_final= data.frame('artista','cancion','index')

for (k_playlist in 1:dim(dataframe_playlist)){
    variable_auxiliar_playlist=sample_index_playlist[k_playlist]
    
    playlist_semi_final[k_playlist,1]=dataframe_playlist[variable_auxiliar_playlist,1]
    playlist_semi_final[k_playlist,2]=dataframe_playlist[variable_auxiliar_playlist,3]
    playlist_semi_final[k_playlist,3]=k_playlist
    
    contador_de_tiempo=contador_de_tiempo+dataframe_playlist$duration_ms[variable_auxiliar_playlist]
    if(contador_de_tiempo >= 180){
      break
    }
}
```

# Elección de canción y playlist asociada

Finalmente, se ilustran los resultados del algoritmo.

## Canción elegida

La canción  es elegida de manera aleatoria de las canciones que fueron seleccionadas para completar las 3 horas de reproducción. En esta iteración la canción elegida fue:

```{r}
sample_index_cancion <- sample(1:nrow(playlist_semi_final),1, replace = F)
playlist_semi_final[sample_index_cancion,]
```

## Playlist Asociada

La playlist corresponde al resto de canciones que fueron elegidas para completar las tres horas de duración, por lo que la playlist asociada a esa determinada canción elegida es: 

```{r}
playlist_final = playlist_semi_final[!(playlist_semi_final$X.index. == sample_index_cancion),]
playlist_final$X.index.=NULL

playlist_final
```

De esta manera, se finaliza el entregable con una playlist de música tranquila y relajante del tipo instrumental antigua de estilo clásica, sin vocalización. Se destacan las canciones de "Mother Nature Sound FX", quien aparece reiterada veces durante la playlist con música totalmente relajante, en donde incluye sonidos de la naturaleza tales como aves, lluvia, agua, madera, entre otros. "Relaxmycat" se describe en la web como un compositor de música relajante para gatos, lo cual corrobora la composición de esta playlist. Continuando con los integrantes de la playlist, "Christophe Beck" es un compositor de música sin la incorporación de voces, en busca de sinfonías relajantes y de corta duración. A diferencia del resto, sus canciones aparecen en películas bastante actuales tales como "Antman and the Wasp". "J.S. Bach" fue un  compositor, organista, clavecinista, director de orquesta, violinista, violagambista, maestro de capilla, cantor y profesor alemán del período barroco. Fue el miembro más importante de una de las familias de músicos más destacadas de la historia, con más de 35 compositores famosos. Su incorporación en la playlist señala la integración de música tanto de la época como de épocas antiguas, pero siempre enfatizando en la búsqueda de tranquilidad con instrumentos clásicos y la nula vocalización. Semejante al anterior artista mencionado, "Camille Saint-Saëns" figura con su música clásica entre los años 1835 y 1921, agregando este tipo de música a la playlist.  Por último, "Sergei Rachmaninoff" y "Ludwig van Beethoven" introducen en la playlist el mismo toque de música clásica antigua y en instrumentos clásicos como el órgano para agregar tranquilidad.

De esta manera se puede concluir que la playlist asociada corresponde a una playlist compuesta por canciones de gran similitud y debido al orden en que se generaron los clusters (es decir, realizar clusters y quedarse con los que presenten mayor agrupación entre los integrantes del cluster) se puede inferir que estos artistas con las canciones que integran la playlist, son las canciones dentro de la muestra que más similitud contienen. 

El resultado debería cambiar si no se utiliza la semilla planteada en los primeros comandos.
